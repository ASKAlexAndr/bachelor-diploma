\section{Машинное обучение} \label{ML}

\subsection{Понятие искусственной нейронной сети}

Машинное обучение – раздел исследований в сфере ИИ, в основе которых лежат методы разработки систем способных к обучению. Алгоритмы машинного обучения эффективно себя показывают в задачах, в которых требуется у заранее подготовленных (обучающих) данных определить общие признаки и по ним идентифицировать новые данные. В проектировании таких, обучающихся, систем часто применяют искуственные нейронные сети. 

Искусственная нейронная сеть (ИНС) – компьютерная модель, в основе которой лежат принципы работы биологической нейронной сети - совокупности связанных между собой нервных клеток - нейронов. Каждый нейрон имеет набор входных связей - синапсов, по которым он получает информацию, представленную в виде импульсов, от других нейронов. По полученным данным нейрон формирует своё состояние и, с помощью аксона, сообщает его другим нейронам, обеспечивая функционирование системы. В процессе формирования системы одни нейронные связи укрепляются, а другие ослабляются, обеспечивая обучаемость сети.
\addimghere{biological-neuron}{0.5}{Типичная структура биологического нейрона}{biological-neuron}

Искусственный нейрон представляет собой упрощенную модель биологического нейрона. Принцип его работы представлен на рисунке \ref{simple-neuron}. Сначала нейрон получает n-мерные вектор входных значений $X=(x_{1},...,x_{n})$ и вектор весов $W=(w_{1},...,w_{n})$, обозначающий <<укрепленность>> межнейронных связей. Вычесляется сумма произведения входных значений и весов $s_j$. Затем к полученному результату применяется \hyperref[sec:activation]{функция активации} $\varphi$. В некоторых случаях, к сумме прибавляется величина смещения $b_j$.
\input{thesis/extra/neuron.tex}

Множества нейронов формируют слои, слои в свою очередь формируют нейронную сеть. Входной слой получает данные, обрабатывает и передает нейронам скрытого слоя. Аналогично срабатывет каждый последующий слой вплоть до выходного. 
\input{thesis/extra/neural-network.tex}

Нейросети с большим количеством скрытых слоев называется глубокими. Область машинного обучения, в которой используются глубокие нейронные сети называется - глубоким обучением.     

\subsection{Активационная функция}
\label{sec:activation}
Взвешенная сумма входов представляет собой линейную комбинацию, из чего следует, что независимо от количества слоев, значения выходного слоя зависят только от входов первого слоя. 
Активационная функция нейрона обеспечивает нормализацию посчитанной суммы и нелинейность нейронной сети. Для многих моделей нейронных сетей также требуется, чтобы активационная функция была монотонной и непрерывно-дифференцируемой на всей области определения.

Существует большое количество функций активации. Наиболее распространенные из них представлены в таблице \ref{actvs}.

\input{thesis/extra/activation-functions.tex}

Отдельно стоит упоминуть функцию Softmax. Эта функция часто применяется на последнем слое глубоких нейронных сетей в задачах классификации. Пусть последний слой сети содержит N нейронов, каждый из которых соответствует некоторому классу. Тогда значение выхода i-го нейрона вычисляется по формуле: 
\[
    y_i=\frac{e^{z_i}}{\sum\limits_{j=1}^{N}e^{z_j}}
\]
Таким образом, результат каждого нейрона будет принимать значения из диапазона $[0,1]$, а их сумма равна 1. По итогу, сеть выдаст вероятности отношения входных данных к заданным классам.
\\\\
\subsection{Обучение нейронных сетей}
Под обучением нейронных сетей подразумевается подбор значений весов связей для эффективного решения поставленной задачи. Изначально, веса устанавливаются случайно. Затем, в процессе прогона через сеть тестовых данных, веса корректируются так, чтобы в конечном итоге сеть выдавала правильные ответы. 

В процессе обучения сети используются тренировочные наборы данных, которые разбиваются на пакеты меньшего размера. Проход через сеть всех пакетов называется эпохой.

Для того, чтобы контролировать процесс обучения необходимо как-то оценивать работу сети. Для этого вводится функия потерь (функция стоимости), которая вычисляет разницу между правильными и полученными результатами и формирует некоторое численное значение, характеризующее величину ошибки работы сети. Таким образом задача обучения сети сводится к задаче минимизации данной функции. В таблице \ref{loss_funcs} указаны наиболее часто используемые функции потерь, где $y_i$ – ожидаемое значение i-го нейрона, $x_i$ – полученное значение i-го нейрона, n – количество выходных нейронов.

 \input{thesis/extra/loss-functions.tex}

Одним из популярных методов в обучении глубоких нейронных сетей является алгоритм обратного распространения ошибки, основанный на градиентном спуске. 

Пусть сеть имеет L слоев, $a^l$, $w_{}^l$, $b^l$ - векторы значений, весов и смещений нейронов на $l$-м слое.. Также имеется N обучающих пар (x,y). 
В процессе обучения циклично происходят следующие итерации: 

\begin{enumerate}
    \item На вход сети подается вектор x из обучающего множества, для каждого слоя вычислить значения: \hfill $z^l = w^la^{l-1}+b^l и a^l = \sigma(z^l)$
    \item Вычислить значение функции стоимости: \hfill $C = \frac{1}{2}\sum_j{(y_j-a_j^L)^2}$
    \item Вычислить значения ошибок выходного слоя: \hfill $\delta_j^L=\frac{\delta C}{\delta a_j^L}\sigma'(z_j^L)$
    \item Вычислить ошибки для каждого предыдущего слоя: \hfill $\delta_j^l=\sum_k{w_{kj}^{l+1} \delta_k^{l+1}\sigma'(z_j^l)}$
    \item Вычислить градиент функции стоимости: \hfill $\frac{\delta C}{\delta w_{jk}^l} = a_k^{l-1} \delta_j^l$ % и $\frac{\delta C}{\delta b_j^l} = \delta_j^l$
    \item Обновить веса связей: \hfill $w_{ij}^l=w_{ij}^l-\mu\frac{\delta C}{\delta w_{jk}^l},\hspace{1em} 0<\mu \leqslant 1$
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% http://neuralnetworksanddeeplearning.com/chap2.html
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Данный метод относится к алгоритмам \textbf{обучения с учителем} - наиболее распространенному типу обучения, в котором сеть учится на заранее размеченных данных, где уже известны правильные ответы.

Существует и другие подходы к обучению нейронных сетей:

\textbf{Обучение с подкреплением} - метод, который подразумевает наличие некоторой окружающей среды в которой действует сеть. Такая среда реагирует на действия модели и подает ей определенные сигналы.  

\textbf{Обучение без учителя} - обучение при котором сеть заренее не располагает правильными ответами и самостоятельно ищет общие и отличительные признаки у входных данных. 

\textbf{Генетические алгоритмы обучения} - алгоритмы, имитирующие эволюционные механизмы развития биологической популяции, выступают как альтернатива алгоритму обратного распространения ошибки. Значение произвольного весового коэффициента в нейронной сети называется геном. Гены формируют хромосомы, а хромосомы - популяцию. Дальше, в пределах одной эпохи с определенными вероятностями происходит: 
 \begin{itemize}
     \item Cкрещивание хромосом - формирование новой хромосомы из генов двух других
     \item мутация - случайное изменение произвольного гена
     \item приспособление - (хромосомы показавшие худшие результаты уничтожаются из популяции.
 \end{itemize}

% \subsection{Рекуррентные нейронные сети}
% Одним из минусов указанных выше моделей является их неспособность анализировать наборы данных, в которых важен порядок, например при работе с текстом или видеорядом. Эту проблему решают рекуррентные нейронные сети, особенностью которых является наличие обратных связей, которые позволяют передавать информацию на следующий шаг системы.
% \input{thesis/extra/RNN.tex}

\subsection{Проблемы обучения глубоких нейронных сетей}

В алгоритмах обучения на основе метода обратного распространения ошибки. Значение ошибки зависит от производной функции активации, так при использовании сигмоидной функции активации значение ошибки при распространении от последнего слоя к первому очень быстро уменьшается, тем самым веса на ранних слоях корректируются слабо. Аналогично можно столкнуться и с проблемой взрывного градиента, когда значение ошибки становится очень большим. Простой способ решения такой проблемы - использование функции ReLU, производная которой принимает значения либо 0 либо 1.

Переобучение сети - проблема когда что сеть обучается хорошо анализировать объекты только из обучающей выборки и плохо работает с новыми данными. Одним из методов решения этой проблемы является Dropout, суть которого заключается в следующем: На каждой итерации обучения нейроны с некоторой вероятностью выключаются. Для оставшихся нейронов происходит обучение методом обратного распространения ошиюки, после чего нейроны возвращаются в сеть. 

\input{thesis/extra/dropout.tex}

% Ещё одной проблемой, замедляющей обучение сети, является 

\clearpage